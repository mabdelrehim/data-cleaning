{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_unwanted_symbols(sent: str):\n",
    "    \"\"\"\n",
    "\n",
    "    remove unwanted symbols\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    symbols = '[\\(\\)|/#&%*\\[\\]ã€Šã€‹\\{\\}~<=>`â€”â€”-]+'\n",
    "    sent = re.sub(symbols, \"\", sent)\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_unwanted_symbols(\"Hel{l}o...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ultimate-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install camel-tools\n",
    "!camel_data full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attempted-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ØªØ±ÙŠØ¯ Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ù…Ø§Ø°Ø§ Ø£Ù†Øª ØªØ¹ØªÙ‚Ø¯ 122?\n"
     ]
    }
   ],
   "source": [
    "import camel_tools as camel\n",
    "from camel_tools.utils.stringutils import force_encoding, force_unicode\n",
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "def ar_clean(mapper, sent: str):\n",
    "    sent = force_unicode(sent)\n",
    "    return mapper.map_string(sent)\n",
    "\n",
    "\n",
    "mapper = CharMapper.builtin_mapper('arclean')\n",
    "print(ar_clean(mapper, \"ØªÙØ±ÙŠØ¯Ù Ø¥Ø®Ù’Ø¨Ø§Ø±ÙŠ Ù…Ø§Ø°Ø§ Ø£Ù†Øª ØªÙØ¹ØªÙ‚Ø¯Ù 122ØŸ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bulgarian-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø§Ø±ÙŠØ¯', 'Ø§Ù†', 'Ø§Ø°Ù‡Ø¨', 'Ø§Ù„Ù‰', 'Ø§Ù„Ø³ÙŠÙ†Ù…Ø§', ',', 'Ø§Ù„Ù…ØªØ­Ù']\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "print(simple_word_tokenize(\"Ø§Ø±ÙŠØ¯ Ø§Ù† Ø§Ø°Ù‡Ø¨ Ø§Ù„Ù‰ Ø§Ù„Ø³ÙŠÙ†Ù…Ø§, Ø§Ù„Ù…ØªØ­Ù\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "circular-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mohamed is going to visit  jhjh\n"
     ]
    }
   ],
   "source": [
    "# Go to URL\n",
    "def rmURL(line):\n",
    "    import  re\n",
    "    pattern = re.compile(r'http[a-zA-Z0-9.?/&=:]*')\n",
    "    return pattern.sub('', line.strip())\n",
    "print(rmURL(\"mohamed is going to visit http://mabdelrehim.com/about jhjh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proved-miami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in /anaconda/envs/data-env2/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: colorama in /anaconda/envs/data-env2/lib/python3.8/site-packages (from demoji) (0.4.4)\n",
      "Requirement already satisfied: requests<3.0.0 in /anaconda/envs/data-env2/lib/python3.8/site-packages (from demoji) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/data-env2/lib/python3.8/site-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/data-env2/lib/python3.8/site-packages (from requests<3.0.0->demoji) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/data-env2/lib/python3.8/site-packages (from requests<3.0.0->demoji) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/data-env2/lib/python3.8/site-packages (from requests<3.0.0->demoji) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "serious-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.16 seconds)\n",
      "Writing emoji data to /home/nlp-mt/.demoji/codes.json ...\n",
      "... OK\n",
      "'I bet you didn't know that :person raising hand:, :person raising hand:â€â™‚ï¸, and :woman raising hand: are three different emojis.\n"
     ]
    }
   ],
   "source": [
    "import demoji\n",
    "demoji.download_codes()\n",
    "print(demoji.replace_with_desc(\"'I bet you didn't know that ğŸ™‹, ğŸ™‹â€â™‚ï¸, and ğŸ™‹â€â™€ï¸ are three different emojis.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nominated-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "productive-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(src_sent: str, trg_sent: str, src_lang: str, trg_lang: str , fasttext_model):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Takes a parallel pair of sentences in a parallel corpus and returns whether to remove the pair\n",
    "    or not based on\n",
    "        1- A large portion of the source sentence is in a language other than the source language\n",
    "        2- A large portion of the target sentence is in a language other than the target language\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    detected_src, confidence_src = fasttext_model.predict(src_sent, k=1)\n",
    "    detected_src = detected_src[0].replace(\"__label__\", \"\")\n",
    "    confidence_src = confidence_src[0]\n",
    "\n",
    "    detected_trg, confidence_trg = fasttext_model.predict(trg_sent, k=1)\n",
    "    detected_trg = detected_trg[0].replace(\"__label__\", \"\")\n",
    "    confidence_trg = confidence_trg[0]\n",
    "\n",
    "    #print(detected_src,\"|||||||\",detected_src)\n",
    "\n",
    "    if (not (detected_src == src_lang)) or (not (detected_trg == trg_lang)):\n",
    "        print(f'src {detected_src} trg {detected_trg}')\n",
    "        return True\n",
    "    elif (confidence_src < 0.85) or (confidence_trg < 0.85):\n",
    "        print(f'confidence src {confidence_src} confidence trg {confidence_trg}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'src {detected_src} trg {detected_trg}')\n",
    "        print(f'confidence src {confidence_src} confidence trg {confidence_trg}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "responsible-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-19 13:09:34--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: 'lid.176.bin.1'\n",
      "\n",
      "lid.176.bin.1       100%[===================>] 125.18M  13.5MB/s    in 9.5s    \n",
      "\n",
      "2021-02-19 13:09:44 (13.2 MB/s) - 'lid.176.bin.1' saved [131266198/131266198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sporting-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src zh trg ar\n",
      "confidence src 0.9829135537147522 confidence trg 0.9992139935493469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "fasttext_model = fasttext.load_model('lid.176.bin')\n",
    "lang_detect(src_sent=\"è¯Šæ‰€å°†ç»§ç»­éµå¾ª PHE åˆ¶å®šçš„ COVID-19 æ–¹æ¡ˆï¼Œæœ‰æ„ŸæŸ“é£é™©è€…åº”è½¬è¯Šè‡³æ§åˆ¶é€šé“ï¼Œè€Œä¸æ˜¯äº²è‡ªå‰å¾€è¯Šæ‰€å°±è¯Šã€‚\", trg_sent=\"Ø³ØªØ¸Ù„ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø§Øª ØªØªØ¨Ø¹ Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ Ù‡ÙŠØ¦Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø© ÙÙŠ Ø¥Ù†Ø¬Ù„ØªØ±Ø§ Ù„ÙƒÙˆÙÙŠØ¯-19 ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø¹Ø±Ø¶ÙŠÙ† Ù„Ø®Ø·Ø± Ø§Ù„Ø¥ØµØ§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø¯ÙˆÙ‰ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¬Ø¨ ÙˆØ¶Ø¹ Ø¥Ø´Ø§Ø±Ø§Øª ØªØ­Ø°ÙŠØ±ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯Ù‡Ù… ÙÙŠ Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø­ØªÙˆØ§Ø¡ØŒ Ø¹ÙˆØ¶Ù‹Ø§ Ø¹Ù† Ø­Ø¶ÙˆØ±Ù‡Ù… Ø¬Ø³Ø¯ÙŠÙ‘Ù‹Ø§.ØŸ\", src_lang=\"zh\", trg_lang=\"ar\" , fasttext_model=fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "japanese-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models into /anaconda/envs/data-env2/lib/python3.8/site-packages/laserembeddings/data\n",
      "\n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
      "\n",
      "âœ¨ You're all set!\n"
     ]
    }
   ],
   "source": [
    "!pip install laserembeddings\n",
    "!pip install laserembeddings[zh]\n",
    "!pip install laserembeddings[ja]\n",
    "!python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "laughing-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "laser = Laser()\n",
    "\n",
    "def laser_filter(laser_model, src: str, trg: str, src_lang: str, trg_lang: str):\n",
    "    \"\"\"\n",
    "    \n",
    "    The laser model tries to embed sentences that are of the same meaning with similar embeddings\n",
    "    basic experimentation tells that sentences with cosine similarity score >= 0.8 are of acceptable quality\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings_a = laser_model.embed_sentences(\n",
    "        [src],\n",
    "        lang=src_lang)\n",
    "    embeddings_b = laser_model.embed_sentences(\n",
    "        [trg],\n",
    "        lang=trg_lang)\n",
    "    similatiry_score = cosine_similarity(embeddings_a, embeddings_b).flatten()[0]\n",
    "    if similarity_score < 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "raised-interview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8142539]\n"
     ]
    }
   ],
   "source": [
    "get_laser_similarity(\"There should be no memory\", \"åº”è¯¥ æ²¡æœ‰ ä»€ä¹ˆ è®°å¿† äº†\", \"en\", \"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civilian-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 34/2735 [02:09<2:51:33,  3.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-860cbe40dc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     }\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mbatched_laser_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'---- Number of samples in before cleaning: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-860cbe40dc7d>\u001b[0m in \u001b[0;36mbatched_laser_filter\u001b[0;34m(laser_model, sources, targets, src_lang, trg_lang, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             embeddings_a.extend(laser_model.embed_sentences(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             lang=src_lang))\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/site-packages/laserembeddings/laser.py\u001b[0m in \u001b[0;36membed_sentences\u001b[0;34m(self, sentences, lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msre_performance_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# see https://bugs.python.org/issue37723\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             sentence_tokens = [\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_lang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/site-packages/laserembeddings/laser.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msre_performance_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# see https://bugs.python.org/issue37723\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             sentence_tokens = [\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_lang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             ]\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/site-packages/laserembeddings/preprocessing.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# MOSES_TOKENIZER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# see: https://github.com/facebookresearch/LASER/issues/55#issuecomment-480881573\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         text = self.tokenizer.tokenize(text,\n\u001b[0m\u001b[1;32m     93\u001b[0m                                        \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                                        \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/site-packages/sacremoses/tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, aggressive_dash_splits, return_str, escape, protected_patterns)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# Separate special characters outside of IsAlnum character set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAD_NOT_ISALNUM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0;31m# Aggressively splits dashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maggressive_dash_splits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/data-env2/lib/python3.8/re.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# literal replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import fasttext\n",
    "from laserembeddings import Laser\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def batched_laser_filter(laser_model, sources, targets, src_lang: str, trg_lang: str, batch_size=4096):\n",
    "    \"\"\"\n",
    "    \n",
    "    The laser model tries to embed sentences that are of the same meaning with similar embeddings\n",
    "    basic experimentation tells that sentences with cosine similarity score >= 0.8 are of acceptable quality\n",
    "    \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    embeddings_a = []\n",
    "    with tqdm(total=len(sources) // batch_size) as pbar:\n",
    "        while i < len(sources):\n",
    "            embeddings_a.extend(laser_model.embed_sentences(\n",
    "            sources[i:i+batch_size],\n",
    "            lang=src_lang))\n",
    "            i = i + batch_size\n",
    "            pbar.update(1)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"---- Calculated laser embeddings for {len(sources)} sentences from source language in {elapsed // 60},  mins , {elapsed} % 60,  secs\")\n",
    "\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    embeddings_b = []\n",
    "    with tqdm(total=len(targets) // batch_size) as pbar:\n",
    "        while i < len(targets):\n",
    "            embeddings_b.extend(laser_model.embed_sentences(\n",
    "            targets[i:i+batch_size],\n",
    "            lang=trg_lang))\n",
    "            i = i + batch_size\n",
    "            pbar.update(1)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"---- Calculated laser embeddings for {len(targets)} sentences from target language in {elapsed // 60},  mins , {elapsed} % 60,  secs\")\n",
    "\n",
    "    similarities = []\n",
    "    start = time.time()\n",
    "    for a, b in tqdm(zip(embeddings_a, embeddings_b)):\n",
    "        similarities.append(cosine_similarity([a], [b]))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"---- Calculated cosine for {len(targets)} pairs in {elapsed // 60},  mins , {elapsed} % 60,  secs\")\n",
    "\n",
    "\n",
    "laser = Laser()\n",
    "data_file_src = open(os.path.join(\"../open-subtitles-2018\", \"data\" + \".\" + \"zh\"), \"r\")\n",
    "data_file_trg = open(os.path.join(\"../open-subtitles-2018\", \"data\" + \".\" + \"en\"), \"r\")\n",
    "\n",
    "src_sents = data_file_src.readlines()\n",
    "trg_sents = data_file_trg.readlines()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"zh\": src_sents,\n",
    "        \"en\": trg_sents\n",
    "    }\n",
    ")\n",
    "batched_laser_filter(laser, src_sents, trg_sents, \"zh\", \"en\")\n",
    "print(f'---- Number of samples in before cleaning: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "focused-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63764677]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity([[1, 1, 1]], [[1, 10, 100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-correlation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-env2",
   "language": "python",
   "name": "conda-env-data-env2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
